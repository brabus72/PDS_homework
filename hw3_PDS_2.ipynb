{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Для чего и в каких случаях полезны различные варианты усреднения лейблов целевой переменной: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее часто используемой метрикой для оценки качества мультиклассовой класификации для несбалансированных наборов данных является мультиклассовый вариант $f_{1}$-меры. Идея, лежащая в основе мультиклассовой $f_{1}$-меры, заключается в вычислении одной бинарной $f_{1}$-меры для каждого класса, интерессующий класс становится положительным, а все остальные - отрицательными классами. Затем эти $f_{1}$-меры для каждого класса усредняются с использованием одной из следующих стратегий.\n",
    "\n",
    "- **micro** - усреднение вычисляет общее количество ложно положительных примеров, ложно отрицательных примеров и истинно положительных примеров по всем классам, а затем вычисляет точность, полноту $f_{1}$-меру с помощью этих показателей;\n",
    "\n",
    "- **macro** - усреднение вычисляет $f_{1}$-меры для каждого класса и находит их не взвешенное среднее. Всем классам, независимо от их размера, присваивается одинаковый вес;\n",
    "\n",
    "- **weighted** - усреднение вычисляет $f_{1}$-меры для каждого класса и находит их среднее, взвешенное по поддержке (количеству фактических примеров для каждого класса). Эта стратегия используется в классификационном отчёте по умолчанию.\n",
    "\n",
    "Если нам необходимо присвоить одинаковый вес каждому примеру, рекомендуется использовать микроусреднение $f_{1}$-меры; если нам необходимо присвоить одинаковый вес каждому классу, рекомендуется использовать макроусреднение $f_{1}$-меры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, сравним алгоритмы градиентного бустинга над решающими деревьями встречаются xgboost, lightgbm и catboost.\n",
    "\n",
    " - **xgboost** - самая распространенная реализация градиентного бустинга. Появившись в 2014 г., уже к 2016-му она завоевала немалую популярность. Для выбора разбиения используют сортировку и модели, основанные на анализе гистограмм. Реализация алгоритма была разработана для эффективности вычислительных ресурсов времени и памяти. Цель проекта заключалась в том, чтобы наилучшим образом использовать имеющиеся ресурсы для обучения модели. К особенностям реализации алгоритма можем отнести следующее:\n",
    "1) различные стратегии обработки пропущенных данных;\n",
    "2) блочная структура для поддержки распараллеливания обучения деревьев;\n",
    "3) продолжение обучения для дообучения на новых данных.\n",
    " \n",
    " - **lightGBM** - фреймворк с реализацией градиентного бустинга от корпорации Microsoft, вышедший в 2017 г. Для выбора критерия разбиения используется Gradient-based One-Side Sampling (GOSS). Имеются методы работы с категориальными признаками, т.е. с признаками, которые явно не выражаются числом (например, имя автора или марка машины). Является частью проекта Microsoft DMTK, посвященного реализации подходов машинного обучения для .Net. В LightGBM нет структуры данных для узла. Вместо этого в структуре данных дерева Tree содержатся массивы значений, где в качестве индекса выступает номер узла. Значения в листьях также хранятся в отдельных массивах. LightGBM поддерживает категориальные признаки. Поддержка осуществляется с помощью битового поля, которое хранится в cat_threshold_ для всех узлов. В cat_boundaries_ хранит, к какому узлу какая часть битового поля соответствует. Поле threshold_ для категориального случая переводится в int и соответсвует индексу в cat_boundaries_ для поиска начала битового поля. Этот подход имеет один существенный недостаток: если категориальный признак может принимать большие значения, например 100500, то для каждого решающего правила для этого признака будет создано битовое поле размером до 12564 байт! Поэтому желательно перенумеровать значения категориальных признаков, чтобы они шли непрерывно от 0 до максимального значения.\n",
    " \n",
    " - **catBoost** - разработка компании Яндекс, вышедшая, как и LightGBM, в 2017 г. Реализует особый подход к обработке категориальных признаков (основанный на target encoding, т.е. на подмене категориальных признаков статистиками на основе предсказываемого значения). К тому же алгоритм содержит особый подход к построению дерева, который показал лучшие результаты. Проведенное нами сравнение показало, что данный алгоритм лучше других работает прямо «из коробки», т.е. без настройки каких-либо параметров. Главным преимуществом **catBoost** является то, что она одинаково хорошо работает «из коробки» как с числовыми признаками, так и с категориальными.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
